# Counts the proportion of the word in the total number of words featured by each author
fear_words$proportion <- ifelse(fear_words$author == "EAP", fear_words$n/frequency$EAP, ifelse(fear_words$author == "HPL", fear_words$n/frequency$HPL, fear_words$n/frequency$MWS))
png("../figs/Fear.png")
ggplot(fear_words) +
geom_col(aes(reorder(word, all, FUN = min), proportion, fill = author)) +
xlab(NULL) +
coord_flip() +
facet_wrap(~ author) +
theme(legend.position = "none")
dev.off()
spooky_wrd_bigrams <- unnest_tokens(spooky, bigram, text, token = "ngrams", n = 2)
head(spooky_wrd_bigrams)
spooky_wrd_bigrams <- unnest_tokens(spooky, bigram, text, token = "ngrams", n = 2)
spooky_wrd_bigrams <- separate(spooky_wrd_bigrams, bigram, c("word1", "word2"), sep = " ")
spooky_wrd_bigrams <- spooky_wrd_bigrams %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
packages.used <- c("ggplot2", "dplyr", "tidytext", "wordcloud", "stringr", "ggridges", "SnowballC", "tidyr", "widyr", "plyr")
# check packages that need to be installed.
packages.needed <- setdiff(packages.used, intersect(installed.packages()[,1], packages.used))
# install additional packages
if(length(packages.needed) > 0) {
install.packages(packages.needed, repos = 'http://cran.us.r-project.org')
}
library(ggplot2)
library(plyr)
library(dplyr)
library(tidytext)
library(wordcloud)
library(stringr)
library(ggridges)
library(SnowballC)
library(tidyr)
library(widyr)
source("../lib/multiplot.R")
spooky <- read.csv('../data/spooky.csv', as.is = TRUE)
head(spooky)
summary(spooky)
sum(is.na(spooky))
spooky$author <- as.factor(spooky$author)
spooky_wrd <- unnest_tokens(spooky, word, text)
head(spooky_wrd)
spooky_wrd_stop <- spooky_wrd
spooky_wrd <- anti_join(spooky_wrd, stop_words, by = "word")
head(spooky_wrd)
EAP <- sum(spooky_wrd$author == "EAP")
HPL <- sum(spooky_wrd$author == "HPL")
MWS <- sum(spooky_wrd$author == "MWS")
frequency <- data.frame(EAP, HPL, MWS, row.names = "Frequency of words")
p1 <- ggplot(spooky) +
geom_bar(aes(author, fill = author)) +
theme(legend.position = "none")
spooky$sen_length <- str_length(spooky$text)
p2 <- ggplot(spooky) +
geom_density_ridges(aes(sen_length, author, fill = author)) +
scale_x_log10() +
theme(legend.position = "none") +
labs(x = "Sentence length [# characters]")
spooky_wrd$word_length <- str_length(spooky_wrd$word)
p3 <- ggplot(spooky_wrd) +
geom_density(aes(word_length, fill = author), bw = 0.05, alpha = 0.3) +
scale_x_log10() +
labs(x = "Word length [# characters]")
layout <- matrix(c(1, 2, 1, 3), 2, 2, byrow = TRUE)
multiplot(p1, p2, p3, layout = layout)
# Counts number of times each author used each word.
author_words <- count(group_by(spooky_wrd, word, author))
# Counts number of times each word was used.
all_words    <- rename(count(group_by(spooky_wrd, word)), all = n)
author_words <- left_join(author_words, all_words, by = "word")
author_words <- arrange(author_words, desc(all))
author_words <- ungroup(head(author_words, 60))
# Counts the proportion of the word in the total number of words featured by each author
author_words$proportion <- ifelse(author_words$author == "EAP", author_words$n/frequency$EAP, ifelse(author_words$author == "HPL", author_words$n/frequency$HPL, author_words$n/frequency$MWS))
ggplot(author_words) +
geom_col(aes(reorder(word, all, FUN = min), proportion, fill = author)) +
xlab(NULL) +
coord_flip() +
facet_wrap(~ author) +
theme(legend.position = "none")
#Count each word grouped by author
words_EAP <- count(group_by(spooky_wrd[spooky_wrd$author == "EAP",], word))$word
freqs_EAP <- count(group_by(spooky_wrd[spooky_wrd$author == "EAP",], word))$n
words_HPL <- count(group_by(spooky_wrd[spooky_wrd$author == "HPL",], word))$word
freqs_HPL <- count(group_by(spooky_wrd[spooky_wrd$author == "HPL",], word))$n
words_MWS <- count(group_by(spooky_wrd[spooky_wrd$author == "MWS",], word))$word
freqs_MWS <- count(group_by(spooky_wrd[spooky_wrd$author == "MWS",], word))$n
wordcloud(words_EAP, freqs_EAP, max.words = 30, color = c("yellow", "orange", "red"))
wordcloud(words_HPL, freqs_HPL, max.words = 30, color = c("yellow", "orange", "red"))
wordcloud(words_MWS, freqs_MWS, max.words = 30, color = c("yellow", "orange", "red"))
# Calculate the tf-idf for each word
tf_idf <- spooky_wrd %>%
count(author, word) %>%
bind_tf_idf(word, author, n) %>%
arrange(desc(tf_idf))
# Give the top 30 tf-idfs
tf_idf <- head(tf_idf, 30)
ggplot(tf_idf) +
geom_col(aes(reorder(word, tf_idf), tf_idf, fill = author)) +
labs(x = NULL, y = "tf-idf") +
theme(legend.position = "none") +
facet_wrap(~ author, ncol = 3, scales = "free") +
coord_flip() +
labs(y = "TF-IDF values")
# Counts number of times each author used each word.
author_words_stop <- count(group_by(spooky_wrd_stop, word, author))
# Counts number of times each word was used.
all_words_stop    <- rename(count(group_by(spooky_wrd_stop, word)), all = n)
author_words_stop <- left_join(author_words_stop, all_words_stop, by = "word")
author_words_stop <- arrange(author_words_stop, desc(all))
author_words_stop <- ungroup(head(author_words_stop, 60))
# Counts the proportion of the word in the total number of words featured by each author
author_words_stop$proportion <- ifelse(author_words_stop$author == "EAP", author_words_stop$n/frequency$EAP, ifelse(author_words_stop$author == "HPL", author_words_stop$n/frequency$HPL, author_words_stop$n/frequency$MWS))
ggplot(author_words_stop) +
geom_col(aes(reorder(word, all, FUN = min), proportion, fill = author)) +
xlab(NULL) +
coord_flip() +
facet_wrap(~ author) +
theme(legend.position = "none")
spooky_wrd_stem <- spooky_wrd
spooky_wrd_stem$word <- wordStem(spooky_wrd_stem$word)
# Counts number of times each author used each word.
author_words_stem <- count(group_by(spooky_wrd_stem, word, author))
# Counts number of times each word was used.
all_words_stem    <- rename(count(group_by(spooky_wrd_stem, word)), all = n)
author_words_stem <- left_join(author_words_stem, all_words_stem, by = "word")
author_words_stem <- arrange(author_words_stem, desc(all))
author_words_stem <- ungroup(head(author_words_stem, 60))
# Counts the proportion of the word in the total number of words featured by each author
author_words_stem$proportion <- ifelse(author_words_stem$author == "EAP", author_words_stem$n/frequency$EAP, ifelse(author_words_stem$author == "HPL", author_words_stem$n/frequency$HPL, author_words_stem$n/frequency$MWS))
ggplot(author_words_stem) +
geom_col(aes(reorder(word, all, FUN = min), proportion, fill = author)) +
xlab(NULL) +
coord_flip() +
facet_wrap(~ author) +
theme(legend.position = "none")
accented <- grep('à|á|â|ä|æ|ã|å|ā|è|é|ê|ë|ē|ė|ęî|ï|í|ī|į|ì|ô|ö|ò|ó|œ|ø|ō|õ|û|ü|ù|ú|ū', spooky_wrd$word)
spooky_wrd_accented <- spooky_wrd[accented,]
# Counts number of times each author used each word.
author_words_accented <- count(group_by(spooky_wrd_accented, word, author))
author_words_accented <- arrange(author_words_accented, desc(n))
# Gets the top 30
author_words_accented <- group_by(author_words_accented, author)
author_words_accented <- top_n(author_words_accented, 30, word)
# Counts the proportion of the word in the total number of words featured by each author
author_words_accented$proportion <- ifelse(author_words_accented$author == "EAP", author_words_accented$n/frequency$EAP, ifelse(author_words_accented$author == "HPL", author_words_accented$n/frequency$HPL, author_words_accented$n/frequency$MWS))
ggplot(author_words_accented) +
#geom_col(aes(reorder(word, all, FUN = min), proportion, fill = author)) +
geom_col(aes(reorder(word, proportion), proportion, fill = author)) +
labs(x = NULL, y = "Proportion") +
coord_flip() +
facet_wrap(~ author, ncol = 2, scales = "free") +
theme(legend.position = "none")
EAP_accented <- sum(spooky_wrd_accented$author == "EAP")
HPL_accented <- sum(spooky_wrd_accented$author == "HPL")
MWS_accented <- sum(spooky_wrd_accented$author == "MWS")
# Counts the proportion of the accented word in the total number of words featured by each author
accented <- data.frame(Author = c("EAP", "HPL", "MWS"), Proportion_Accented = c(EAP_accented/frequency$EAP, HPL_accented/frequency$HPL, MWS_accented/frequency$MWS))
ggplot(data = accented, aes(x = Author, y = Proportion_Accented, fill = Author)) +
geom_bar(stat="identity")
# Get the bigrams
spooky_wrd_bigrams <- unnest_tokens(spooky, bigram, text, token = "ngrams", n = 2)
# Separate the bigrams into its own column
spooky_wrd_bigrams <- separate(spooky_wrd_bigrams, bigram, c("word1", "word2"), sep = " ")
# Filter out the stop words
spooky_wrd_bigrams <- spooky_wrd_bigrams %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
#Unite the bigrams again
spooky_wrd_bigrams <- unite(spooky_wrd_bigrams, bigram, word1, word2, sep = " ")
#Count the number of unique bigrams for each author
bigram_counts <- count(spooky_wrd_bigrams, author, bigram, sort = T)
# This gives us the top 30 counts
bigram_counts <- top_n(bigram_counts, 30, n)
ggplot(bigram_counts) +
geom_col(aes(reorder(bigram, n), n, fill = author)) +
labs(x = NULL, y = "counts") +
theme(legend.position = "top", axis.text.x  = element_text(angle=45, hjust=1, vjust=0.9))
# Calculate the tf-idf for each bigram
bigram_tf_idf <- spooky_wrd_bigrams %>%
count(author, bigram) %>%
bind_tf_idf(bigram, author, n) %>%
arrange(desc(tf_idf))
# Give the top 30 tf-idfs
bigram_tf_idf <- head(bigram_tf_idf, 30)
ggplot(bigram_tf_idf) +
geom_col(aes(reorder(bigram, tf_idf), tf_idf, fill = author)) +
labs(x = NULL, y = "tf-idf") +
theme(legend.position = "none") +
facet_wrap(~ author, ncol = 3, scales = "free") +
coord_flip() +
labs(y = "TF-IDF values")
EAP_pairs <- pairwise_count(filter(spooky_wrd, author == "EAP"), word, id, sort = TRUE)
EAP_pairs$author <- "EAP"
HPL_pairs <- pairwise_count(filter(spooky_wrd, author == "HPL"), word, id, sort = TRUE)
HPL_pairs$author <- "HPL"
MWS_pairs <- pairwise_count(filter(spooky_wrd, author == "MWS"), word, id, sort = TRUE)
MWS_pairs$author <- "MWS"
# ddply is used to count pairwise for each author group
pairs <- ddply(spooky_wrd, .(author), pairwise_count, word, id, sort = TRUE)
pairs <- group_by(pairs, author)
# Filtered top 30 for faster computation
pairs <- top_n(pairs, 30, n)
# Remove duplicates by sorting
sorted_pairs <- t(apply(pairs[,2:3], 1, sort))
pairs$item1 <- sorted_pairs[, 1]
pairs$item2 <- sorted_pairs[, 2]
pairs <- pairs[!duplicated(pairs),]
# United Pairs into one column
pairs <- unite(pairs, pairs, item1, item2, sep = " ")
ggplot(pairs) +
geom_col(aes(reorder(pairs, n), n, fill = author)) +
labs(x = NULL, y = "Frequency") +
theme(legend.position = "none") +
facet_wrap(~ author, ncol = 3, scales = "free") +
coord_flip() +
labs(y = "Frequency")
# Keep words that have been classified within the NRC lexicon.
get_sentiments('nrc')
sentiments <- inner_join(spooky_wrd, get_sentiments('nrc'), by = "word")
# Count how many times each sentiment occurs
sentiments_count <- count(group_by(sentiments, sentiment, author))
# Convert to proportion because each author are featured at different amounts
sentiments_count$proportion <- ifelse(sentiments_count$author == "EAP", sentiments_count$n/frequency$EAP, ifelse(sentiments_count$author == "HPL", sentiments_count$n/frequency$HPL, sentiments_count$n/frequency$MWS))
ggplot(sentiments_count) +
geom_col(aes(sentiment, proportion, fill = sentiment)) +
facet_wrap(~ author) +
coord_flip() +
theme(legend.position = "none")
nrc_fear <- filter(get_sentiments('nrc'), sentiment == "fear")
fear <- inner_join(spooky_wrd, nrc_fear, by = "word")
#Counts the number of fear words for each author, as well as the total among all three
fear_words     <- count(group_by(fear, word, author))
fear_words_all <- rename(count(group_by(fear, word)), all = n)
fear_words <- left_join(fear_words, fear_words_all, by = "word")
fear_words <- arrange(fear_words, desc(all))
fear_words <- ungroup(head(fear_words, 60))
# Counts the proportion of the word in the total number of words featured by each author
fear_words$proportion <- ifelse(fear_words$author == "EAP", fear_words$n/frequency$EAP, ifelse(fear_words$author == "HPL", fear_words$n/frequency$HPL, fear_words$n/frequency$MWS))
ggplot(fear_words) +
geom_col(aes(reorder(word, all, FUN = min), proportion, fill = author)) +
xlab(NULL) +
coord_flip() +
facet_wrap(~ author) +
theme(legend.position = "none")
library(reticulate)
# If you are using anaconda, point reticulate to the correct conda environment
# use_condaenv('your-environment')
# for some reason I need to import cv2 and tensorflow before EBImage
# or everything breaks.
cv2 <- reticulate::import('cv2')
# for some reason I need to import cv2 and tensorflow before EBImage
# or everything breaks.
cv2 <- reticulate::import('cv2')
# for some reason I need to import cv2 and tensorflow before EBImage
# or everything breaks.
cv2 <- reticulate::import('cv2')
if(!require("EBImage")){
source("https://bioconductor.org/biocLite.R")
biocLite("EBImage")
}
if(!require("gbm")){
install.packages("gbm")
}
library("EBImage")
library("gbm")
getwd()
if(!require("EBImage")){
source("https://bioconductor.org/biocLite.R")
biocLite("EBImage")
}
if(!require("gbm")){
install.packages("gbm")
}
library("EBImage")
library("gbm")
getwd()
setwd("/Users/Chris/Documents/GitHub/project-2-predictive-modelling-group-5/doc")
# Replace the above with your own path or manually set it in RStudio to where this rmd file is located.
experiment_dir <- "../data/catsDogs/" # This will be modified for different data sets.
img_train_dir  <- paste(experiment_dir, "train/", sep="")
img_test_dir   <- paste(experiment_dir, "test/", sep="")
run.cv            <- TRUE # run cross-validation on the training set
K                 <- 5    # number of CV folds
run.feature.train <- TRUE # process features for training set
run.test          <- TRUE # run evaluation on an independent test set
run.feature.test  <- TRUE # process features for test set
label_train <- read.table(paste(experiment_dir, "train_label.txt", sep = ""), header = F)
label_train
label_train <- read.table(paste(experiment_dir, "train_label.txt", sep = ""), header = F)
label_train <- as.numeric(unlist(label_train) == "cat")
?feature
??feature
load("/Users/Chris/Documents/GitHub/project-2-predictive-modelling-group-5/data/catsDogs/train-features/pet1.jpg.sift.Rdata")
features
load("/Users/Chris/Documents/GitHub/project-2-predictive-modelling-group-5/data/catsDogs/train-features/pet2.jpg.sift.Rdata")
View(features)
load("/Users/Chris/Documents/GitHub/project-2-predictive-modelling-group-5/data/catsDogs/train-features/pet1.jpg.sift.Rdata")
load("/Users/Chris/Documents/GitHub/project-2-predictive-modelling-group-5/data/catsDogs/train-features/pet1.jpg.sift.Rdata")
load("/Users/Chris/Documents/GitHub/project-2-predictive-modelling-group-5/data/catsDogs/train-features/pet1.jpg.sift.Rdata")
load("/Users/Chris/Documents/GitHub/project-2-predictive-modelling-group-5/data/catsDogs/train-features/pet3.jpg.sift.Rdata")
load("/Users/Chris/Documents/GitHub/project-2-predictive-modelling-group-5/data/catsDogs/train-features/pet1.jpg.sift.Rdata")
View(features)
View(bob)
View(bob_new)
rm(list=ls())
load("/Users/Chris/Documents/GitHub/project-2-predictive-modelling-group-5/output/feature_train.RData")
load("/Users/Chris/Documents/GitHub/project-2-predictive-modelling-group-5/data/catsDogs/train-features/pet1.jpg.sift.Rdata")
load("/Users/Chris/Documents/GitHub/project-2-predictive-modelling-group-5/data/catsDogs/train-features/pet2.jpg.sift.Rdata")
head(features)
head(keypoints)
if(!require("EBImage")){
source("https://bioconductor.org/biocLite.R")
biocLite("EBImage")
}
if(!require("gbm")){
install.packages("gbm")
}
library("EBImage")
library("gbm")
setwd("/Users/Chris/Documents/GitHub/project-2-predictive-modelling-group-5/doc")
experiment_dir <- "../data/catsDogs/" # This will be modified for different data sets.
img_train_dir  <- paste(experiment_dir, "train/", sep="")
img_test_dir   <- paste(experiment_dir, "test/", sep="")
run.cv            <- TRUE # run cross-validation on the training set
K                 <- 5    # number of CV folds
run.feature.train <- FALSE # process features for training set
run.test          <- FALSE # run evaluation on an independent test set
run.feature.test  <- FALSE # process features for test set
run.cv            <- TRUE # run cross-validation on the training set
K                 <- 5    # number of CV folds
run.feature.train <- FALSE # process features for training set
run.test          <- TRUE # run evaluation on an independent test set
run.feature.test  <- FALSE # process features for test set
model_values <- seq(3, 11, 2)
model_labels <- paste("GBM with depth =", model_values)
model_values
model_labels
label_train <- read.table(paste(experiment_dir, "train_label.txt", sep = ""), header = F)
label_train <- as.numeric(unlist(label_train) == "cat")
label_test <- label_train[1501:]
label_train
label_test <- label_train[1501:2000]
label_train <- label_train[1:1500]
label_test
?`gbm-package`
load("/Users/Chris/Documents/GitHub/project-2-predictive-modelling-group-5/output/feature_train.RData")
dat_train
features[1]
features
setwd("/Users/Chris/Documents/GitHub/project-2-predictive-modelling-group-5/doc")
experiment_dir <- "../data/catsDogs/" # This will be modified for different data sets.
img_train_dir  <- paste(experiment_dir, "train/", sep="")
img_test_dir   <- paste(experiment_dir, "test/", sep="")
experiment_dir
source("../lib/cross_validation.R")
for(i in 1:length(file.sources)) load(file.sources[[i]])
###Load features
file.sources = list.files(c("../data/pets/train-features/"),
pattern="*.Rdata")
for(i in 1:length(file.sources)) load(file.sources[[i]])
file.sources
{
load(file.sources[[i]])
}
load("/Users/Chris/Documents/GitHub/project-2-predictive-modelling-group-5/data/pets/train-features/pet1.jpg.sift.Rdata")
load("/Users/Chris/Documents/GitHub/project-2-predictive-modelling-group-5/data/pets/train-features/pet1.jpg.sift.Rdata")
load("/Users/Chris/Documents/GitHub/project-2-predictive-modelling-group-5/data/pets/train-features/pet1.jpg.sift.Rdata")
file.sources
length(file.sources)
load(file.sources[[1]])
load(file.sources[1])
load(file = paste0("../data/pets/train-features/", file.sources[1])
)
load(file = paste0("../output/feature_", "zip", "_", "test", ".RData"))
load(file = paste0("../data/pets/train-features/", file.sources[1]))
load(file = paste0("../data/pets/train-features/", file.sources[i]))
load(file = paste0("../data/pets/train-features/", file.sources[1]))
load(paste0("../data/pets/train-features/", file.sources[1]))
getwd()
setwd("/Users/Chris/Documents/GitHub/project-2-predictive-modelling-group-5/doc")
getwd()
file.sources
file.sources[1]
file.sources[1]
load(file.sources[1])
setwd("~/Documents/GitHub/project-2-predictive-modelling-group-5")
getwd()
setwd("~/Documents/GitHub/project-2-predictive-modelling-group-5/doc")
setwd("~/Documents/GitHub/project-2-predictive-modelling-group-5/doc")
getwd()
experiment_dir <- "../data/pets/" # This will be modified for different data sets.
img_train_dir  <- paste(experiment_dir, "train/", sep="")
img_test_dir   <- paste(experiment_dir, "test/", sep="")
run.cv            <- TRUE # run cross-validation on the training set
K                 <- 5    # number of CV folds
run.feature.train <- FALSE # process features for training set
run.test          <- TRUE # run evaluation on an independent test set
run.feature.test  <- FALSE # process features for test set
model_values <- seq(3, 11, 2)
model_labels <- paste("GBM with depth =", model_values)
label_train <- read.table(paste(experiment_dir, "train_label.txt", sep = ""), header = F)
label_train <- as.numeric(unlist(label_train) == "cat")
label_test <- label_train[1501:2000]
label_train <- label_train[1:1500]
load(file = paste0("../data/pets/train-features/", file.sources[1]))
###Load features, from piazza
file.sources = list.files(c("../data/pets/train-features/"),
pattern="*.Rdata")
for(i in 1:length(file.sources))
{
load(file.sources[[i]])
load(file = paste0("../data/pets/train-features/", file.sources[i]))
}
###Load features, from piazza
file.sources = list.files(c("../data/pets/train-features/"),
pattern="*.Rdata")
for(i in 1:length(file.sources))
{
load(file = paste0("../data/pets/train-features/", file.sources[i]))
}
dat_train <- features[1:1500,]
dim(features)
features
load("/Users/Chris/Documents/GitHub/project-2-predictive-modelling-group-5/data/pets/train-features/pet1.jpg.sift.Rdata")
load("/Users/Chris/Documents/GitHub/project-2-predictive-modelling-group-5/data/pets/train-features/pet2.jpg.sift.Rdata")
load("/Users/Chris/Documents/GitHub/project-2-predictive-modelling-group-5/data/pets/train-features/pet3.jpg.sift.Rdata")
length(file.sources)
{
load(paste0("../data/pets/train-features/", file.sources[i]))
}
###Load features, from piazza
file.sources = list.files(c("../data/pets/train-features/"),
pattern="*.Rdata")
for(i in 1:length(file.sources))
{
load(paste0("../data/pets/train-features/", file.sources[i]))
}
dat_train <- features[1:1500,]
load(paste0("../data/pets/train-features/", file.sources[1]))
load(paste0("../data/pets/train-features/", file.sources[2]))
load(paste0("../data/pets/train-features/", file.sources[3]))
load(paste0("../data/pets/train-features/", file.sources[4]))
load(paste0("../data/pets/train-features/", file.sources[5]))
load(paste0("../data/pets/train-features/", file.sources[6]))
load(paste0("../data/pets/train-features/", file.sources[7]))
load(paste0("../data/pets/train-features/", file.sources[8]))
load(paste0("../data/pets/train-features/", file.sources[9]))
load(paste0("../data/pets/train-features/", file.sources[10]))
load("/Users/Chris/Documents/GitHub/project-2-predictive-modelling-group-5/data/pets/train-features/pet1.jpg.sift.Rdata")
features
read.csv("sift_test.csv")
sift <- read.csv("sift_test.csv")
head(sift)
sift <- read.csv(paste(experiment_dir,"sift_train.csv",sep = ""), header=TRUE, as.is = TRUE)
sift <- read.csv(paste(experiment_dir,"sift_train.csv",sep = ""), header=TRUE, as.is = TRUE)
sift <- read.csv(paste(experiment_dir,"sift_test.csv",sep = ""), header=TRUE, as.is = TRUE)
getwd()
sift <- read.csv(paste(experiment_dir,"sift_test.csv",sep = ""), header=TRUE, as.is = TRUE)
read.table(paste(experiment_dir, "train_label.txt", sep = ""), header = F)
sift <- read.csv(paste(experiment_dir, "sift_test.csv", sep = ""), header=TRUE, as.is = TRUE)
library(readr)
dataset <- read_csv(NULL)
View(dataset)
features
dim(features)
load("/Users/Chris/Documents/GitHub/project-2-predictive-modelling-group-5/data/pets/train-features/pet2.jpg.sift.Rdata")
dim(features)
load("/Users/Chris/Documents/GitHub/project-2-predictive-modelling-group-5/output/feature_train.RData")
dat_train
dim(dat_train)
label_train <- read.table(paste(experiment_dir, "train_label.txt", sep = ""), header = F)
label_train <- as.numeric(unlist(label_train) == "cat")
if(!run.test)
{
train_index <- sample(1:nrow(label),0.75*nrow(dat))
label_test <- label_train[-train_index]
label_train <- label_train[train_index]
}
train_index
run.cv            <- TRUE # run cross-validation on the training set
K                 <- 5    # number of CV folds
run.feature.train <- FALSE # process features for training set
run.test          <- FALSE # run evaluation on an independent test set
run.feature.test  <- FALSE # process features for test set
label_train <- read.table(paste(experiment_dir, "train_label.txt", sep = ""), header = F)
label_train <- as.numeric(unlist(label_train) == "cat")
if(!run.test)
{
train_index <- sample(1:nrow(label),0.75*nrow(dat))
label_test <- label_train[-train_index]
label_train <- label_train[train_index]
}
label_train <- read.table(paste(experiment_dir, "train_label.txt", sep = ""), header = F)
label_train <- as.numeric(unlist(label_train) == "cat")
if(!run.test)
{
train_index <- sample(1:nrow(label_train),0.75*nrow(label_train))
label_test <- label_train[-train_index]
label_train <- label_train[train_index]
}
label_train
nrow(label_train)
label_train <- read.table(paste(experiment_dir, "train_label.txt", sep = ""), header = F)
label_train <- as.numeric(unlist(label_train) == "cat")
if(!run.test)
{
train_index <- sample(1:length(label_train),0.75*length(label_train))
label_test <- label_train[-train_index]
label_train <- label_train[train_index]
}
train_index
